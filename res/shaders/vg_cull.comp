layout(binding = 0, set = 0) buffer TaskBuffer
{
    // Node data structure
    NodeWork taskNodes[];
    ClusterWork taskClusters[];
    uint nextNodeTask;
    uint nextClusterTask;
    uint totalNodeTasks;
    uint totalClusterTasks;
};

void CullCluster()
{

}

bool _LoadNode(uint nodeIndex)
{
    // TODO load node data from taskNodes[nodeIndex]
    return false;
}

uint _GetBatchClusterCount(uint _offset)
{
    return 0; // TODO
}

#define BARRIER memoryBarrierShared(); barrier();
#define MAX_NODES_PER_WORKGROUP 16
#define NODES_COUNT_IN_TREE 1024 // TODO
#define CLUSTERS_COUNT_IN_TREE 256 // TODO
#define CLUSTERS_COUNT_PER_WORKGROUP 64
#define NODE_TO_PROCESS_COUNT 0 // TODO

shared uint g_workgroupPassedNodeCount;    // count of nodes passed cull in this workgroup
shared uint g_workgroupNodeOffset;         // offset of the first node task for this workgroup
shared uint g_workgroupNodeReadyMask;      // bitmask of loaded nodes in current batch -> workgroup size should be <= 32
shared uint g_workgroupClusterBatchOffset; // offset of the first cluster task for this workgroup
shared uint g_nodesRemaining; // nodes remaining to process in the whole mesh
shared uint g_workgroupBatchClusterCount;

void CullNode(uint _availableNodeCount, uint _threadID)
{
    if (_threadID >= _availableNodeCount) return;

    Node nodeToProcess; // TODO load from process data array
    float errorScale = 1.0; // TODO compute error scale based on distance to camera
}

void HierarchyTraversal()
{
    const uint threadID = gl_GlobalInvocationID.x;
    
    // same for all threads in workgroup
    bool workgroupProcessNode = true;
    uint workgroupDoneNodeCount = MAX_NODES_PER_WORKGROUP; 
    
    // local clone of workgroup shared variables, same for all threads in workgroup
    uint workgroupNodeOffset = 0;
    uint workgroupClusterBatchOffset = 0xFFFFFFFFu;
    uint workgroupNodeReadyMask = 0;
    uint workgroupBatchClusterCount = 0;

    while(true)
    {
        BARRIER;
        if (threadID == 0)
        {
            g_workgroupPassedNodeCount = 0;
            g_workgroupNodeReadyMask = 0;
        }
        BARRIER;

        if (workgroupProcessNode)
        {
            if (workgroupDoneNodeCount == MAX_NODES_PER_WORKGROUP)
            {
                workgroupDoneNodeCount = 0;

                // fetch workgroup node tasks from hierarchy
                if (threadID == 0)
                {
                    g_workgroupNodeOffset = atomicAdd(nextNodeTask, MAX_NODES_PER_WORKGROUP);
                }

                BARRIER;
                workgroupNodeOffset = g_workgroupNodeOffset;

                if (workgroupNodeOffset >= NODES_COUNT_IN_TREE)
                {
                    workgroupProcessNode = false; // No more node tasks, try to do cluster tasks
                    continue; // All threads within this workgroup will take the same branch
                }
            }

            const uint nodeToProcess = workgroupNodeOffset + workgroupDoneNodeCount + threadID;
            bool nodeReady = (threadID + workgroupDoneNodeCount) < MAX_NODES_PER_WORKGROUP && nodeToProcess < NODES_COUNT_IN_TREE;
            
            // check if node loaded
            if (nodeReady)
            {
                // TODO: load node data, fill data to process data array
                nodeReady = _LoadNode(nodeToProcess);
                
                if (nodeReady)
                {
                    // mark loaded nodes
                    atomicOr(g_workgroupNodeReadyMask, 1 << threadID);
                }
            }
            BARRIER;
            workgroupNodeReadyMask = g_workgroupNodeReadyMask;

            // Process loaded nodes
            if (workgroupNodeReadyMask & 1u) // we only process first contiguously loaded nodes
            {
                uint frontReadyNodeCount = findLSB(~workgroupNodeReadyMask);

                // TODO process node cull
                // CullNode(frontReadyNodeCount, threadID);

                workgroupDoneNodeCount += frontReadyNodeCount;

                // TODO erase processed data in process data array, -> neccessary?
                continue; // all threads within this workgroup will take the same branch
            }
        }

        // We reach here when:
        // Some nodes don't loaded yet -> workgroupProcessNode == true
        // or all node tasks done, which means all valid cluster tasks should be written into buffer now -> workgroupProcessNode == false

        // fetch workgroup cluster batch task if we first reach here or last batch was finished
        if (workgroupClusterBatchOffset == 0xFFFFFFFFu)
        {
            // fill workgroup cluster tasks
            if (threadID == 0)
            {
                g_workgroupClusterBatchOffset = atomicAdd(nextClusterTask, 1);
            }
            BARRIER;
            workgroupClusterBatchOffset = g_workgroupClusterBatchOffset;
        }

        // All cluster tasks done, finish
        if (!workgroupProcessNode && workgroupClusterBatchOffset >= CLUSTERS_COUNT_IN_TREE) break;

        if (threadID == 0)
        {
            g_nodesRemaining = NODE_TO_PROCESS_COUNT;
            g_workgroupBatchClusterCount = _GetBatchClusterCount(workgroupClusterBatchOffset);
        }
        
        BARRIER;
        workgroupBatchClusterCount = g_workgroupBatchClusterCount;
        workgroupProcessNode = (g_nodesRemaining == 0) ? false : workgroupProcessNode;

        if (!workgroupProcessNode && workgroupBatchClusterCount == 0) break;

        // process cluster batch cull
        if (// hungry? -> process full batch, else, hang task when nodes cull finished
            (workgroupProcessNode && workgroupBatchClusterCount == CLUSTERS_COUNT_PER_WORKGROUP)
            || (!workgroupProcessNode && workgroupBatchClusterCount > 0))
        {
            // TODO process cluster batch cull
            // CullCluster(workgroupClusterBatchOffset);

            // mark batch as done -> fetch new batch next iteration
            workgroupClusterBatchOffset = 0xFFFFFFFFu;
        }
    }
}